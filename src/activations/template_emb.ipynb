{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Sentence-context embeddings for a list of target words â€” LAST LAYER ONLY.\n",
    "\n",
    "For each word:\n",
    "  1) Build a template sentence: \"What is the meaning of the word {word}?\"\n",
    "  2) Run through the model with output_hidden_states=True\n",
    "  3) Locate token span(s) of the word inside the sentence\n",
    "  4) Pool subword vectors **from the last hidden layer only**\n",
    "  5) Save a single vector of shape (hidden_size,) as .npy\n",
    "\n",
    "Output: OUT_DIR / <model_short> / \"sentence_last\" / \"<word>.npy\"\n",
    "\"\"\"\n",
    "\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "USE_BF16 = torch.cuda.is_available()\n",
    "\n",
    "POOL = \"mean\"  # \"mean\" | \"first\" | \"last\"  (for subword pooling within the span)\n",
    "SENTENCE_TEMPLATE = \"What is the meaning of the word {word}?\"\n",
    "\n",
    "WORDS_CSV = \"/Users/tikhomirova/PycharmProjects/Paper ICRL/continuous_data.csv\" #I TAKE THE WORDS FROM THIS CSV, YOU WILL NEED YOUR OWN OBVIOUSLY - MB REFRACTOR TO TAKE TXT FILE IF NEEDED\n",
    "FALLBACK_C4_JSON = \"C4_sentences.json\"\n",
    "WORDS_LIMIT = None\n",
    "\n",
    "OUT_DIR = Path(\"./per_layer_word_embeddings\")\n",
    "\n",
    "MODELS = [\n",
    "    {\"name\": \"JinaEmb-v3\", \"id\": \"jinaai/jina-embeddings-v3\", \"short\": \"jina_v3\", \"trc\": True, \"type\": \"encoder\"},\n",
    "    # add more if needed\n",
    "]\n",
    "\n",
    "# --------------- UTILS ------------------\n",
    "def clean_mem() -> None:\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def sanitize(text: str) -> str:\n",
    "    return \"\".join(ch if ch.isalnum() or ch in \"._-\" else \"_\" for ch in text)\n",
    "\n",
    "def find_subsequence_tokens(big: List[str], small: List[str]) -> Tuple[int, int]:\n",
    "    n, m = len(big), len(small)\n",
    "    if m == 0 or m > n:\n",
    "        return -1, -1\n",
    "    for i in range(n - m + 1):\n",
    "        if big[i:i + m] == small:\n",
    "            return i, i + m\n",
    "    return -1, -1\n",
    "\n",
    "# ------------- LOAD MODELS --------------\n",
    "def load_encoder(model_id: str, trust_remote_code: bool):\n",
    "    cfg = AutoConfig.from_pretrained(model_id, trust_remote_code=trust_remote_code)\n",
    "    cfg.output_hidden_states = True\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=trust_remote_code)\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_id,\n",
    "        config=cfg,\n",
    "        trust_remote_code=trust_remote_code,\n",
    "        torch_dtype=(torch.bfloat16 if (USE_BF16 and torch.cuda.is_available()) else None),\n",
    "        low_cpu_mem_usage=True,\n",
    "    ).to(DEVICE).eval()\n",
    "\n",
    "    if tok.pad_token is None:\n",
    "        if tok.eos_token is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        else:\n",
    "            tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        try:\n",
    "            model.resize_token_embeddings(len(tok))\n",
    "        except Exception:\n",
    "            pass\n",
    "    tok.padding_side = \"right\"\n",
    "    return tok, model\n",
    "\n",
    "def load_decoder(model_id: str, trust_remote_code: bool):\n",
    "    cfg = AutoConfig.from_pretrained(model_id, trust_remote_code=trust_remote_code)\n",
    "    cfg.output_hidden_states = True\n",
    "    cfg.use_cache = False\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=trust_remote_code)\n",
    "    if tok.pad_token is None:\n",
    "        if tok.eos_token is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        else:\n",
    "            tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    tok.padding_side = \"right\"\n",
    "\n",
    "    common = dict(trust_remote_code=trust_remote_code, device_map=\"auto\", low_cpu_mem_usage=True)\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, dtype=dtype, **common)\n",
    "    except TypeError:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype, **common)\n",
    "\n",
    "    if getattr(model.config, \"use_cache\", True):\n",
    "        model.config.use_cache = False\n",
    "\n",
    "    model.eval()\n",
    "    return tok, model\n",
    "\n",
    "# --------- SENTENCE EMBEDDINGS ----------\n",
    "@torch.inference_mode()\n",
    "def embed_last_layer_in_sentence(\n",
    "    word: str,\n",
    "    tok,\n",
    "    model,\n",
    "    pool: str = POOL,\n",
    "    template: str = SENTENCE_TEMPLATE,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a sentence with 'word', run the model, and return a single vector\n",
    "    pooled over the word's token span **from the last hidden layer only**.\n",
    "    Returns: np.ndarray of shape (hidden_size,)\n",
    "    \"\"\"\n",
    "    sentence = template.format(word=word)\n",
    "\n",
    "    enc = tok(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        return_offsets_mapping=True,\n",
    "        return_special_tokens_mask=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    token_indices: List[int] = []\n",
    "    offsets = enc.get(\"offset_mapping\")\n",
    "    specials = enc.get(\"special_tokens_mask\")\n",
    "\n",
    "    # try robust char-span mapping\n",
    "    if offsets is not None:\n",
    "        offs = offsets[0].tolist()\n",
    "        spmsk = (specials[0].tolist() if specials is not None else [0] * len(offs))\n",
    "        a = sentence.lower().find(word.lower())\n",
    "        if a >= 0:\n",
    "            b = a + len(word)\n",
    "            for i, ((x, y), sp) in enumerate(zip(offs, spmsk)):\n",
    "                if sp == 1 or y <= x:\n",
    "                    continue\n",
    "                if not (y <= a or x >= b):  # overlap\n",
    "                    token_indices.append(i)\n",
    "\n",
    "    # fallback: token subsequence\n",
    "    if not token_indices:\n",
    "        ids_full = tok(sentence, add_special_tokens=True)[\"input_ids\"]\n",
    "        ids_word = tok(word, add_special_tokens=False)[\"input_ids\"]\n",
    "        toks_full = tok.convert_ids_to_tokens(ids_full)\n",
    "        toks_word = tok.convert_ids_to_tokens(ids_word)\n",
    "        start, end = find_subsequence_tokens(toks_full, toks_word)\n",
    "        if start >= 0:\n",
    "            token_indices = list(range(start, end))\n",
    "\n",
    "    # fallback: interior non-special tokens\n",
    "    if not token_indices:\n",
    "        attn = enc[\"attention_mask\"][0].tolist()\n",
    "        nz = [i for i, m in enumerate(attn) if m == 1]\n",
    "        token_indices = nz[1:-1] or nz\n",
    "\n",
    "    # to device\n",
    "    tens = {k: v.to(DEVICE) for k, v in enc.items() if torch.is_tensor(v)}\n",
    "    tens.pop(\"offset_mapping\", None)\n",
    "    tens.pop(\"special_tokens_mask\", None)\n",
    "\n",
    "    # forward\n",
    "    if torch.cuda.is_available():\n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**tens, output_hidden_states=True)\n",
    "    else:\n",
    "        out = model(**tens, output_hidden_states=True)\n",
    "\n",
    "    # --- ONLY LAST LAYER ---\n",
    "    if getattr(out, \"hidden_states\", None) is not None:\n",
    "        last = out.hidden_states[-1]         # [1, seq_len, hidden]\n",
    "    else:\n",
    "        last = out.last_hidden_state         # [1, seq_len, hidden]\n",
    "\n",
    "    # pool subwords in the span\n",
    "    token_vecs = last[0, token_indices, :]    # [k, hidden]\n",
    "    if pool == \"mean\":\n",
    "        v = token_vecs.mean(dim=0)\n",
    "    elif pool == \"first\":\n",
    "        v = token_vecs[0]\n",
    "    elif pool == \"last\":\n",
    "        v = token_vecs[-1]\n",
    "    else:\n",
    "        raise ValueError(\"POOL must be 'mean' | 'first' | 'last'\")\n",
    "\n",
    "    return v.detach().cpu().to(torch.float32).numpy()  # (hidden,)\n",
    "\n",
    "# --------------- MAIN -------------------\n",
    "def load_words() -> List[str]:\n",
    "    if WORDS_CSV and Path(WORDS_CSV).exists():\n",
    "        df = pd.read_csv(WORDS_CSV)\n",
    "        words = df.iloc[:, 0].astype(str).tolist()\n",
    "        return words[:WORDS_LIMIT] if WORDS_LIMIT is not None else words\n",
    "\n",
    "    if Path(FALLBACK_C4_JSON).exists():\n",
    "        with open(FALLBACK_C4_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        words = list(data.keys())\n",
    "        return words[:WORDS_LIMIT] if WORDS_LIMIT is not None else words\n",
    "\n",
    "    raise FileNotFoundError(\"No WORDS_CSV or FALLBACK_C4_JSON found\")\n",
    "\n",
    "def process_model(model_spec: dict, words: List[str]) -> None:\n",
    "    name = model_spec[\"name\"]\n",
    "    model_id = model_spec[\"id\"]\n",
    "    short = model_spec[\"short\"]\n",
    "    trust_remote_code = model_spec[\"trc\"]\n",
    "    model_type = model_spec[\"type\"]\n",
    "\n",
    "    out_dir = OUT_DIR / short / \"sentence_last\"  # <-- ONLY LAST LAYER\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nðŸš€ Loading {name} ({model_id}) â†’ {out_dir}\")\n",
    "    try:\n",
    "        if model_type == \"encoder\":\n",
    "            tok, mdl = load_encoder(model_id, trust_remote_code)\n",
    "        else:\n",
    "            tok, mdl = load_decoder(model_id, trust_remote_code)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Skipping {name} â€” load failed: {e}\")\n",
    "        clean_mem()\n",
    "        return\n",
    "\n",
    "    total = len(words)\n",
    "    for i, w in enumerate(words, 1):\n",
    "        fname = sanitize(w) or f\"w_{i}\"\n",
    "        out_path = out_dir / f\"{fname}.npy\"\n",
    "\n",
    "        if out_path.exists():\n",
    "            if i % 200 == 0:\n",
    "                print(f\"  [{i}/{total}] {w} (cached)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            vec = embed_last_layer_in_sentence(w, tok, mdl).astype(np.float16, copy=False)  # (hidden,)\n",
    "            np.save(out_path, vec)\n",
    "        except Exception as e:\n",
    "            print(f\"  [{i}/{total}] {w} â€” sentence (last layer) FAILED: {e}\")\n",
    "            continue\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  [{i}/{total}] {w} âœ“ (hidden={vec.shape[0]})\")\n",
    "\n",
    "    del mdl, tok\n",
    "    clean_mem()\n",
    "    print(f\"âœ… Finished {name} â€” unloaded.\")\n",
    "\n",
    "def main() -> None:\n",
    "    words = load_words()\n",
    "    print(f\"ðŸ“š Words in this run: {len(words)}\")\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    for spec in MODELS:\n",
    "        process_model(spec, words)\n",
    "    print(\"\\nâœ… All models finished (sentence, last layer only). Output:\", OUT_DIR.resolve())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
