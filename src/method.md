# Methodology: Predicting Explicit Judgments from Latent Semantic Structure

### Overview
To assess the semantic grounding of Large Language Models (LLMs), we evaluated the extent to which a model's latent semantic structure could linearly predict psycholinguistic norms. This approach was applied in two contexts: (1) **External Alignment**, predicting ground-truth human ratings, and (2) **Self-Consistency**, predicting the model's own explicit ratings. For each model, we constructed a **Semantic Embedding Space ($X$)**, derived from the model's distributional behavior (either log-likelihoods of human associations or generated association counts) transformed into a dense vector space via Positive Pointwise Mutual Information (PPMI) and Singular Value Decomposition (SVD). We then trained a series of Ridge Regression models to map this latent space to **Scalar Norm Ratings ($y$)** (e.g., Valence, Concreteness). The predictive performance ($R^2$) serves as a metric of representational qualityâ€”quantifying how well the geometry of the model's embedding space encodes the psychological dimensions governing human concepts or its own declarative outputs.

### Implementation Details
* **Semantic Representation ($X$):** 300-dimensional dense vectors derived from the model's probability distribution over the Small World of Words (SWOW) vocabulary ($\approx$12,000 cues).
* **Target Variable ($y$):** Scalar ratings for 14 psycholinguistic norms. For self-consistency, these were explicitly generated by the model (e.g., *"Rate the concreteness of 'dog' from 1-5"*).
* **Model:** **Ridge Regression** ($L_2$ Regularization), chosen for its robustness in high-dimensional feature spaces and ability to handle multicollinearity among embedding dimensions.
* **Preprocessing:** Input features were standardized (z-scored) using `StandardScaler` to ensure uniform scaling across dimensions. Crucially, the scaler was fit strictly on training folds within the cross-validation loop to prevent data leakage.
* **Hyperparameter Tuning:** We employed a **Nested Cross-Validation** scheme. The regularization strength ($\alpha$) was tuned via efficient Leave-One-Out Cross-Validation (LOOCV) on the training set, searching a dense log-uniform grid.
* **Evaluation Protocol:** Performance was evaluated using a rigorous **Repeated K-Fold Cross-Validation** (5 folds $\times$ 5 repeats = 25 total evaluation runs per norm/model pair). We report the mean Out-of-Sample $R^2$ across these 25 splits to ensure statistical stability.
* **Matching Logic:** For self-consistency tests, Embeddings and Norm Ratings were matched strictly by model identity (e.g., `passive_Llama-3.1-8B_instruct` $\leftrightarrow$ `llama-3.1-8b-instruct`) using an explicit mapping to prevent alignment errors.

### TL;DR: Technical Specifications
* **Model:** Ridge Regression ($L_2$ regularization).
* **Dimensionality:** Inputs $X \in \mathbb{R}^{N \times 300}$ (SVD-reduced); Targets $y \in \mathbb{R}^{N}$ (Scalar ratings).
* **Validation Scheme:** Nested Repeated K-Fold ($k=5$, $n_{repeats}=5$).
* **Hyperparameter Tuning:** Inner Leave-One-Out CV (LOOCV) optimizing $\alpha$.
* **Search Space:** $\alpha \in$ `np.logspace(-3, 5, 20)` ($10^{-3}$ to $10^5$).
* **Preprocessing:** Fold-wise Z-score standardization (`StandardScaler`).