"""
src/evaluation/predict_self_consistency.py

The "Self-Consistency" Judge.
Evaluates how well a model's semantic representations predict its OWN generated psycholinguistic norms.

Method:
- Ridge Regression (L2 Regularization) with Cross-Validation.
- Metric: R^2 (Variance Explained).
- Matching: Embeddings for Model X are used to predict Norms generated by Model X.
"""

import argparse
import pickle
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from sklearn.linear_model import RidgeCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score, KFold
from joblib import Parallel, delayed

# --- CONFIG ---
DEFAULT_ALPHAS = [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000]
CV_FOLDS = 5
MIN_SAMPLES = 50  # Minimum overlapping words to attempt regression

def load_embeddings(pkl_path: Path):
    """Load the dictionary of matrices and mappings."""
    print(f"[Loader] Loading embeddings from {pkl_path}...")
    with open(pkl_path, 'rb') as f:
        data = pickle.load(f)
    return data['embeddings'], data['mappings']

def load_model_norms(norms_dir: Path, allowed_models: list = None) -> dict:
    """
    Load all model norm CSVs from the directory.
    Returns: dict { 'model_name': pd.DataFrame }
    """
    if not norms_dir.exists():
        print(f"[Loader] Directory not found: {norms_dir}")
        return {}
        
    model_norms = {}
    files = sorted(list(norms_dir.glob('*.csv')))
    print(f"[Loader] Found {len(files)} model norm files.")
    
    for fp in files:
        # File name is usually "model-name.csv"
        model_name = fp.stem
        
        if allowed_models:
            if not any(m in model_name for m in allowed_models):
                continue
                
        try:
            df = pd.read_csv(fp, low_memory=False)
            
            # Basic Cleaning
            # Expected columns: norm, word, cleaned_rating
            required = {'norm', 'word', 'cleaned_rating'}
            if not required.issubset(df.columns):
                print(f"[Loader] Skipping {model_name}: Missing columns {required - set(df.columns)}")
                continue
                
            df['word'] = df['word'].astype(str).str.lower().str.strip()
            df['cleaned_rating'] = pd.to_numeric(df['cleaned_rating'], errors='coerce')
            df = df.dropna(subset=['cleaned_rating'])
            
            model_norms[model_name] = df
            print(f"[Loader] Loaded norms for {model_name} ({len(df)} rows)")
            
        except Exception as e:
            print(f"[Loader] Error loading {model_name}: {e}")
            
    return model_norms

def align_data(embedding_mat, cue_to_idx, norm_df, norm_name, verbose: bool = False):
    """
    Intersection of:
    1. Words in the embedding matrix (Rows)
    2. Words in the norm dataset (Rows for specific norm)
    
    Returns: X (features), y (targets), overlapping_words
    """
    sub_df = norm_df[norm_df['norm'] == norm_name].copy()
    
    if sub_df.empty:
        return None, None, None

    # Find overlap
    valid_cues = set(cue_to_idx.keys())
    available_words = set(sub_df['word'].unique())
    overlap = list(valid_cues.intersection(available_words))
    
    # Logging
    norm_vocab_size = len(available_words)
    embedding_vocab_size = len(valid_cues)
    overlap_size = len(overlap)
    overlap_pct = overlap_size / norm_vocab_size if norm_vocab_size > 0 else 0
    
    dropped_emb = embedding_vocab_size - overlap_size
    dropped_norms = norm_vocab_size - overlap_size
    
    if verbose:
        print(f"    > Overlap: {overlap_size}/{norm_vocab_size} ({overlap_pct:.1%})")
        print(f"    > Drop Stats: {dropped_emb} Emb cues unused, {dropped_norms} Norm words missing.")
    
        if len(overlap) > 0:
            print(f"    > Sample Overlap: {overlap[:5]}")
            missing = list(available_words - valid_cues)
            if missing:
                print(f"    > Sample Missing (in Norms but not Emb): {missing[:5]}")
    
    if len(overlap) < MIN_SAMPLES:
        return None, None, None
        
    # Build X and y
    row_indices = [cue_to_idx[w] for w in overlap]
    X = embedding_mat[row_indices]
    
    # Handle duplicates by mean
    rating_map = sub_df.groupby('word')['cleaned_rating'].mean().to_dict()
    y = np.array([rating_map[w] for w in overlap])
    
    return X, y, overlap

def evaluate_embedding(X, y, random_state=42, verbose: bool = False):
    """Run Ridge Regression with Nested CV."""
    # Sanitize X
    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

    model = make_pipeline(
        StandardScaler(),
        RidgeCV(alphas=DEFAULT_ALPHAS, scoring='r2')
    )
    
    cv = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=random_state)
    try:
        scores = cross_val_score(model, X, y, cv=cv, scoring='r2')
        if verbose:
            model.fit(X, y)
            best_alpha = model.named_steps['ridgecv'].alpha_
            print(f"    > Best Alpha: {best_alpha}")
        return scores.mean(), scores.std()
    except Exception as e:
        if verbose:
            print(f"    [Error] Regression failed: {e}")
        return np.nan, np.nan

def _evaluate_single_norm(emb_name, matched_model, X_full, cue_to_idx, norms_df, norm, verbose):
    """Helper for parallel evaluation of a single norm."""
    X, y, overlap = align_data(X_full, cue_to_idx, norms_df, norm, verbose=verbose)
    
    if X is None:
        if verbose:
            print(f"    > Skipped (Insufficient Overlap)")
        return None
        
    mean_r2, std_r2 = evaluate_embedding(X, y, verbose=verbose)
    
    if np.isnan(mean_r2):
        return None
        
    return {
        'embedding_source': emb_name,
        'target_model': matched_model,
        'norm_name': norm,
        'r2_mean': mean_r2,
        'r2_std': std_r2,
        'n_samples': len(overlap)
    }

def run_evaluation_loop(embeddings_dict, mappings, model_norms_dict, output_path, verbose: bool = False, n_jobs: int = 1):
    """
    Main loop.
    Logic:
    For each embedding source:
      1. Identify the model name from the key (e.g. 'passive_qwen-3-32b-instruct' -> 'qwen-3-32b-instruct')
      2. Check if we have norms for this model.
      3. If yes, evaluate on all norms present in that model's file.
    """
    results = []
    cue_to_idx = mappings['cue_to_idx']
    
    emb_keys = [k for k in embeddings_dict.keys() if k != 'mappings']
    print(f"[Judge] Found {len(emb_keys)} embedding sources. Processing with n_jobs={n_jobs}...")
    
    # Progress bar logic is tricky because norms vary per model.
    # We'll just iterate and print.
    
    for emb_name in tqdm(emb_keys, desc="Evaluating Models"):
        # 1. Extract Model Name
        matched_model = None
        for model_key in model_norms_dict.keys():
            if model_key in emb_name:
                matched_model = model_key
                break
        
        if not matched_model:
            if verbose:
                print(f"[Judge] Skipping {emb_name}: No matching norms found.")
            continue
            
        norms_df = model_norms_dict[matched_model]
        all_norms = sorted(norms_df['norm'].unique())
        
        if verbose:
            print(f"\n[Judge] Evaluating {emb_name} on {matched_model} norms ({len(all_norms)} tasks)...")
            
        X_full = embeddings_dict[emb_name]
        
        # Parallelize norms for this embedding
        norm_results = Parallel(n_jobs=n_jobs)(
            delayed(_evaluate_single_norm)(emb_name, matched_model, X_full, cue_to_idx, norms_df, norm, verbose)
            for norm in all_norms
        )
        
        for res in norm_results:
            if res:
                results.append(res)
                
    # Save
    res_df = pd.DataFrame(results)
    res_df.to_csv(output_path, index=False)
    print(f"\n[Judge] Self-Consistency Evaluation complete. Results saved to {output_path}")
    
    if not res_df.empty:
        print("\n--- Leaderboard (Average R^2 across all norms) ---")
        summary = res_df.groupby('embedding_source')['r2_mean'].mean().sort_values(ascending=False)
        print(summary)
    else:
        print("\n[Judge] Warning: No valid results produced.")

def main():
    parser = argparse.ArgumentParser(description="Evaluate Self-Consistency (Model Embeddings vs Model Norms).")
    parser.add_argument('--embeddings_path', type=Path, required=True, help="Pickle from vectorize.py")
    parser.add_argument('--norms_dir', type=Path, required=True, help="Dir containing model norm CSVs")
    parser.add_argument('--output_dir', type=Path, required=True, help="Where to save results.csv")
    parser.add_argument('--models', nargs='*', help="List of model names to evaluate (substring match)")
    parser.add_argument('--verbose', action='store_true', help="Enable verbose logging")
    parser.add_argument('--n_jobs', type=int, default=1, help="Number of parallel jobs")
    args = parser.parse_args()
    
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load Data
    embeddings, mappings = load_embeddings(args.embeddings_path)
    model_norms = load_model_norms(args.norms_dir, allowed_models=args.models)
    
    if not model_norms:
        print("[Judge] No model norms loaded. Exiting.")
        return

    # Run Eval
    output_csv = args.output_dir / "self_consistency_results.csv"
    run_evaluation_loop(embeddings, mappings, model_norms, output_csv, verbose=args.verbose, n_jobs=args.n_jobs)

if __name__ == "__main__":
    import sys
    # Default behavior
    try:
        script_dir = Path(__file__).parent.resolve()
        project_root = script_dir.parent.parent
        
        default_emb = project_root / 'outputs' / 'matrices' / 'embeddings.pkl'
        default_norms_dir = project_root / 'outputs' / 'raw_behavior' / 'model_norms'
        default_out = project_root / 'outputs' / 'results'
    except:
        default_emb = Path('.')
        default_norms_dir = Path('.')
        default_out = Path('.')
        
    if len(sys.argv) == 1:
        sys.argv.extend([
            '--embeddings_path', str(default_emb),
            '--norms_dir', str(default_norms_dir),
            '--output_dir', str(default_out)
        ])
        
    main()
